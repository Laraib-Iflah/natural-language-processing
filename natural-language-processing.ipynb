{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-12T04:40:01.185655Z","iopub.execute_input":"2022-07-12T04:40:01.187109Z","iopub.status.idle":"2022-07-12T04:40:01.218144Z","shell.execute_reply.started":"2022-07-12T04:40:01.186963Z","shell.execute_reply":"2022-07-12T04:40:01.216891Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-07-12T04:40:01.220315Z","iopub.execute_input":"2022-07-12T04:40:01.222325Z","iopub.status.idle":"2022-07-12T04:40:02.460357Z","shell.execute_reply.started":"2022-07-12T04:40:01.222273Z","shell.execute_reply":"2022-07-12T04:40:02.459050Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest=  pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-12T04:40:07.417364Z","iopub.execute_input":"2022-07-12T04:40:07.417764Z","iopub.status.idle":"2022-07-12T04:40:07.489618Z","shell.execute_reply.started":"2022-07-12T04:40:07.417732Z","shell.execute_reply":"2022-07-12T04:40:07.488294Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train.tail(50)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T04:40:13.163676Z","iopub.execute_input":"2022-07-12T04:40:13.164111Z","iopub.status.idle":"2022-07-12T04:40:13.201359Z","shell.execute_reply.started":"2022-07-12T04:40:13.164074Z","shell.execute_reply":"2022-07-12T04:40:13.200090Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8, 6))\nsns.heatmap(train.isnull(), yticklabels = False, cbar = True)\nplt.title(\"Missing values\", fontsize = 14)\nplt.xticks(rotation = 40, fontsize = 12)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-12T04:40:18.961680Z","iopub.execute_input":"2022-07-12T04:40:18.962111Z","iopub.status.idle":"2022-07-12T04:40:19.292109Z","shell.execute_reply.started":"2022-07-12T04:40:18.962076Z","shell.execute_reply":"2022-07-12T04:40:19.290972Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"##The heatmap shows missing data. Some keywords in the tail and head of the data are missing. A lot of locations are also missing.","metadata":{"execution":{"iopub.status.busy":"2022-07-12T04:40:45.158685Z","iopub.execute_input":"2022-07-12T04:40:45.159119Z","iopub.status.idle":"2022-07-12T04:40:45.164043Z","shell.execute_reply.started":"2022-07-12T04:40:45.159083Z","shell.execute_reply":"2022-07-12T04:40:45.163091Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T04:40:46.075791Z","iopub.execute_input":"2022-07-12T04:40:46.076228Z","iopub.status.idle":"2022-07-12T04:40:46.088694Z","shell.execute_reply.started":"2022-07-12T04:40:46.076193Z","shell.execute_reply":"2022-07-12T04:40:46.087291Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Out of 3262 entries, 1105 of them do not have a location","metadata":{"execution":{"iopub.status.busy":"2022-07-12T04:40:47.671592Z","iopub.execute_input":"2022-07-12T04:40:47.672602Z","iopub.status.idle":"2022-07-12T04:40:47.677778Z","shell.execute_reply.started":"2022-07-12T04:40:47.672562Z","shell.execute_reply":"2022-07-12T04:40:47.675886Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#set up stop words for the tweets. These words will be disregarded.\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstpwrd = nltk.corpus.stopwords.words('english')\nmore_stop_words=['some', 'play','game','soccer', \"all\", \"due\", \"to\", \"on\", \"daily\"]\nstpwrd.extend(more_stop_words)\nprint(stpwrd)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T04:40:50.617272Z","iopub.execute_input":"2022-07-12T04:40:50.618036Z","iopub.status.idle":"2022-07-12T04:40:51.579924Z","shell.execute_reply.started":"2022-07-12T04:40:50.617974Z","shell.execute_reply":"2022-07-12T04:40:51.578739Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict #Defaultdict is a container like dictionaries present in the module collections. \n#defining a function to read the words (strings) in text column for each tweet as long as the string is not empty or a single space\n#store as many words from the text column as the value of n_grams \n#return the joined string with each word separated by a space\ndef generate_ngrams(text, n_gram=1):\n    word = [word for word in text.lower().split(' ') if word != '' if word not in stpwrd]\n    ngrams = zip(*[word[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\n\nDISASTER_TWEETS = train['target'] == 1\nN=30\n# Trigrams\n#Create two spearate dictionaries for disaster and non-disaster tweets.\ndisaster_trigrams = defaultdict(int)\nnondisaster_trigrams = defaultdict(int)\n\n#for each tweet in training data where DISASTER_TWEETS=1 (i.e. the twwe which is identified as disaster tweet),read the text column\n#the for each word in the return object of the function generate_ngrams which takes two inputs, tweet and ngram, add the word to disaster_trigrams dictionary.\nfor tweet in train[DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        disaster_trigrams[word] += 1\n\n#this loop keeps the non_disaster tweets (DISASTER_TWEETS != 1)\nfor tweet in train[~DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        nondisaster_trigrams[word] += 1\n        \ndf_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])\n","metadata":{"execution":{"iopub.status.busy":"2022-07-12T04:40:56.989436Z","iopub.execute_input":"2022-07-12T04:40:56.989856Z","iopub.status.idle":"2022-07-12T04:40:57.361701Z","shell.execute_reply.started":"2022-07-12T04:40:56.989821Z","shell.execute_reply":"2022-07-12T04:40:57.360490Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(15,30), dpi=100)\n\nsns.barplot(y=df_disaster_trigrams[0].values[:N], x=df_disaster_trigrams[1].values[:N], ax=axes[0], color='lightcoral')\nsns.barplot(y=df_nondisaster_trigrams[0].values[:N], x=df_nondisaster_trigrams[1].values[:N], ax=axes[1], color='lightskyblue')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=11)\n\naxes[0].set_title(f'Top {N} most common trigrams in Disaster Tweets', fontsize=20)\naxes[1].set_title(f'Top {N} most common trigrams in Non-disaster Tweets', fontsize=20)\n\nplt.show()\n\n#the plots below show the top 30 combinations found in disaster tweets, and what is the frequency with which they appear in the test data.\n","metadata":{"execution":{"iopub.status.busy":"2022-07-12T04:41:07.997295Z","iopub.execute_input":"2022-07-12T04:41:07.997688Z","iopub.status.idle":"2022-07-12T04:41:09.160377Z","shell.execute_reply.started":"2022-07-12T04:41:07.997654Z","shell.execute_reply":"2022-07-12T04:41:09.159210Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#geopy is a Python client for several popular geocoding web services. https://geopy.readthedocs.io/en/stable/ Nominatim is the map platform like google maps that is free to use\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter #the extra.rate_limiter extension is \nimport folium #folium allows us to visualize Puthon-manipulated data on a map\nfrom folium import plugins #they are external plugins used to make the maps prettier\n\nnew_data = pd.DataFrame()\nnew_data['location'] = ((train['location'].value_counts())[:30]).index\nnew_data['count'] = ((train['location'].value_counts())[:30]).values\ngeolocator = Nominatim(user_agent = 'Lulu')\ngeocode = RateLimiter(geolocator.geocode, min_delay_seconds = 1)\nlat = {}\nlong = {}\n\nfor i in new_data['location']:\n    location = geocode(i)\n    lat[i] = location.latitude\n    long[i] = location.longitude\nnew_data['latitude'] = new_data['location'].map(lat)\nnew_data['longitude'] = new_data['location'].map(long)\nmap = folium.Map(location = [10.0, 10.0], tiles = 'openstreetmap', zoom_start = 2)\nmarkers = []\ntitle = '''<h1 align = \"center\" style = \"font-size: 15px\"><b>Top 30 Tweet Locations</b></h1>'''\nfor i, r in new_data.iterrows():\n    loss = r['count']\n    if r['count'] > 0:\n        counts = r['count'] * 0.4\n        folium.CircleMarker([float(r['latitude']), float(r['longitude'])], radius = float(counts), color = 'red', fill = True).add_to(map)\nmap.get_root().html.add_child(folium.Element(title))\nmap\n","metadata":{"execution":{"iopub.status.busy":"2022-07-12T04:41:13.913162Z","iopub.execute_input":"2022-07-12T04:41:13.913562Z","iopub.status.idle":"2022-07-12T04:41:43.788049Z","shell.execute_reply.started":"2022-07-12T04:41:13.913530Z","shell.execute_reply":"2022-07-12T04:41:43.786698Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"duplicates = train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\nduplicates = duplicates[duplicates['target'] > 1]['target']\nduplicates\n","metadata":{"execution":{"iopub.status.busy":"2022-07-12T05:34:02.784884Z","iopub.execute_input":"2022-07-12T05:34:02.785315Z","iopub.status.idle":"2022-07-12T05:34:02.826521Z","shell.execute_reply.started":"2022-07-12T05:34:02.785279Z","shell.execute_reply":"2022-07-12T05:34:02.825574Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"train = train.drop(['location','keyword'],1)\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-07-12T05:47:26.687888Z","iopub.execute_input":"2022-07-12T05:47:26.688343Z","iopub.status.idle":"2022-07-12T05:47:26.705769Z","shell.execute_reply.started":"2022-07-12T05:47:26.688305Z","shell.execute_reply":"2022-07-12T05:47:26.704412Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}